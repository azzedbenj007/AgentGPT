{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/azzedbenj007/AgentGPT/blob/main/Maintenance_predict_V2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Nsnvv_iXNuWa"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sarimax_report.py (version am√©lior√©e avec analyse compl√®te)\n",
        "\n",
        "import os\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "\n",
        "# === 1. Configuration ===\n",
        "MODEL_DIR = 'trained_models'\n",
        "DATA_FILE = 'datac.csv'\n",
        "TIME_COLUMN = 'timestamp'\n",
        "REPORT_DIR = 'sarimax_reports'\n",
        "\n",
        "ALL_NUMERIC_COLUMNS = [\n",
        "    'temperature_oil', 'temperature_winding', 'current',\n",
        "    'voltage', 'humidity', 'dissolved_gas'\n",
        "]\n",
        "\n",
        "# === 2. Pr√©paration ===\n",
        "\n",
        "# Cr√©er un dossier pour les rapports si n√©cessaire\n",
        "os.makedirs(REPORT_DIR, exist_ok=True)\n",
        "\n",
        "# Chargement et pr√©paration des donn√©es\n",
        "print(f\"üì• Chargement des donn√©es depuis {DATA_FILE}...\")\n",
        "try:\n",
        "    df = pd.read_csv(DATA_FILE, parse_dates=[TIME_COLUMN])\n",
        "    df.set_index(TIME_COLUMN, inplace=True)\n",
        "    df.sort_index(inplace=True)\n",
        "    # Remplissage robuste des valeurs manquantes\n",
        "    if df.isnull().values.any():\n",
        "        print(\"   -> Remplissage des valeurs manquantes (ffill, bfill)...\")\n",
        "        df.fillna(method='ffill', inplace=True)\n",
        "        df.fillna(method='bfill', inplace=True)\n",
        "except FileNotFoundError:\n",
        "    print(f\"‚ùå Fichier de donn√©es non trouv√© : {DATA_FILE}\")\n",
        "    exit()\n",
        "\n",
        "print(\"‚úÖ Donn√©es pr√™tes.\")\n",
        "\n",
        "# === 3. G√©n√©ration des Rapports (un par variable) ===\n",
        "\n",
        "for col in ALL_NUMERIC_COLUMNS:\n",
        "    print(f\"\\n{'='*20} RAPPORT COMPLET POUR : {col.upper()} {'='*20}\")\n",
        "\n",
        "    # Initialiser le contenu du rapport texte\n",
        "    report_content = [f\"RAPPORT D'ANALYSE COMPLET POUR LA VARIABLE : {col}\\n\", \"=\"*50, \"\\n\"]\n",
        "\n",
        "    # --- PARTIE 1 : ANALYSE DE LA S√âRIE TEMPORELLE BRUTE ---\n",
        "    report_content.append(\"PARTIE 1 : Analyse de la s√©rie temporelle brute\\n--------------------------------------------\\n\")\n",
        "\n",
        "    # 1a. Statistiques descriptives\n",
        "    desc_stats = df[col].describe().to_string()\n",
        "    report_content.append(\"1a. Statistiques Descriptives:\\n\")\n",
        "    report_content.append(desc_stats + \"\\n\")\n",
        "\n",
        "    # 1b. Test de Stationnarit√© (Augmented Dickey-Fuller)\n",
        "    report_content.append(\"\\n1b. Test de Stationnarit√© (Augmented Dickey-Fuller):\\n\")\n",
        "    adf_result = adfuller(df[col])\n",
        "    report_content.append(f'   - ADF Statistic: {adf_result[0]:.4f}')\n",
        "    report_content.append(f'   - p-value: {adf_result[1]:.4f}')\n",
        "    report_content.append('   - Critical Values:')\n",
        "    for key, value in adf_result[4].items():\n",
        "        report_content.append(f'      - {key}: {value:.4f}')\n",
        "    if adf_result[1] <= 0.05:\n",
        "        report_content.append(\"   - Conclusion : p-value <= 0.05. La s√©rie est probablement stationnaire.\\n\")\n",
        "    else:\n",
        "        report_content.append(\"   - Conclusion : p-value > 0.05. La s√©rie n'est probablement pas stationnaire.\\n\")\n",
        "\n",
        "    # 1c. Visualisation de la s√©rie temporelle\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(df.index, df[col], label=f'Valeurs de {col}')\n",
        "    plt.title(f\"√âvolution temporelle de : {col}\")\n",
        "    plt.xlabel(\"Date\")\n",
        "    plt.ylabel(\"Valeur\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(REPORT_DIR, f\"raw_timeseries_{col}.png\"))\n",
        "    plt.close()\n",
        "\n",
        "    # 1d. Visualisation de la distribution\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.histplot(df[col], kde=True)\n",
        "    plt.title(f\"Distribution des valeurs de : {col}\")\n",
        "    plt.xlabel(\"Valeur\")\n",
        "    plt.ylabel(\"Fr√©quence\")\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(REPORT_DIR, f\"raw_distribution_{col}.png\"))\n",
        "    plt.close()\n",
        "\n",
        "    # 1e. Graphes d'autocorr√©lation (ACF & PACF)\n",
        "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))\n",
        "    plot_acf(df[col], ax=ax1, lags=40, title=f\"Autocorr√©lation (ACF) - {col}\")\n",
        "    plot_pacf(df[col], ax=ax2, lags=40, title=f\"Autocorr√©lation Partielle (PACF) - {col}\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(REPORT_DIR, f\"raw_acf_pacf_{col}.png\"))\n",
        "    plt.close()\n",
        "\n",
        "    print(f\"   -> Analyse de la s√©rie brute pour '{col}' termin√©e.\")\n",
        "\n",
        "    # --- PARTIE 2 : ANALYSE DU MOD√àLE SARIMAX ---\n",
        "    report_content.append(\"\\nPARTIE 2 : Analyse du mod√®le SARIMAX entra√Æn√©\\n---------------------------------------------\\n\")\n",
        "\n",
        "    model_path = os.path.join(MODEL_DIR, f'model_{col}.pkl')\n",
        "    if not os.path.exists(model_path):\n",
        "        warning_msg = f\"‚ùå MOD√àLE MANQUANT pour '{col}'. Analyse du mod√®le ignor√©e.\"\n",
        "        print(f\"   -> {warning_msg}\")\n",
        "        report_content.append(warning_msg)\n",
        "    else:\n",
        "        with open(model_path, 'rb') as f:\n",
        "            model_fit = pickle.load(f)\n",
        "\n",
        "        # 2a. R√©sum√© du mod√®le\n",
        "        summary_text = model_fit.summary().as_text()\n",
        "        report_content.append(\"2a. R√©sum√© statistique du mod√®le:\\n\")\n",
        "        report_content.append(summary_text + \"\\n\")\n",
        "\n",
        "        # 2b. Crit√®res d'information\n",
        "        report_content.append(f\"\\n2b. Crit√®res d'information:\\n   - AIC: {model_fit.aic:.2f}\\n   - BIC: {model_fit.bic:.2f}\\n\")\n",
        "\n",
        "        # 2c. Diagnostic des r√©sidus (4 graphiques en 1)\n",
        "        fig = model_fit.plot_diagnostics(figsize=(15, 12))\n",
        "        fig.suptitle(f\"Diagnostic des r√©sidus du mod√®le pour : {col}\", fontsize=16)\n",
        "        plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
        "        plt.savefig(os.path.join(REPORT_DIR, f\"model_diagnostics_{col}.png\"))\n",
        "        plt.close()\n",
        "        print(f\"   -> Analyse du mod√®le pour '{col}' termin√©e.\")\n",
        "\n",
        "    # --- √âcriture du fichier rapport texte consolid√© ---\n",
        "    report_file_path = os.path.join(REPORT_DIR, f'report_{col}.txt')\n",
        "    with open(report_file_path, 'w', encoding='utf-8') as f:\n",
        "        f.write(\"\\n\".join(report_content))\n",
        "    print(f\"   -> Rapport texte sauvegard√© : {report_file_path}\")\n",
        "\n",
        "\n",
        "print(f\"\\n‚úÖ Tous les rapports ont √©t√© g√©n√©r√©s avec succ√®s dans le dossier '{REPORT_DIR}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-U3ARcd4jNw",
        "outputId": "9c556a8c-0a05-46e0-8be6-efc2c1968a27"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì• Chargement des donn√©es depuis datac.csv...\n",
            "‚úÖ Donn√©es pr√™tes.\n",
            "\n",
            "==================== RAPPORT COMPLET POUR : TEMPERATURE_OIL ====================\n",
            "   -> Analyse de la s√©rie brute pour 'temperature_oil' termin√©e.\n",
            "   -> Analyse du mod√®le pour 'temperature_oil' termin√©e.\n",
            "   -> Rapport texte sauvegard√© : sarimax_reports/report_temperature_oil.txt\n",
            "\n",
            "==================== RAPPORT COMPLET POUR : TEMPERATURE_WINDING ====================\n",
            "   -> Analyse de la s√©rie brute pour 'temperature_winding' termin√©e.\n",
            "   -> Analyse du mod√®le pour 'temperature_winding' termin√©e.\n",
            "   -> Rapport texte sauvegard√© : sarimax_reports/report_temperature_winding.txt\n",
            "\n",
            "==================== RAPPORT COMPLET POUR : CURRENT ====================\n",
            "   -> Analyse de la s√©rie brute pour 'current' termin√©e.\n",
            "   -> Analyse du mod√®le pour 'current' termin√©e.\n",
            "   -> Rapport texte sauvegard√© : sarimax_reports/report_current.txt\n",
            "\n",
            "==================== RAPPORT COMPLET POUR : VOLTAGE ====================\n",
            "   -> Analyse de la s√©rie brute pour 'voltage' termin√©e.\n",
            "   -> Analyse du mod√®le pour 'voltage' termin√©e.\n",
            "   -> Rapport texte sauvegard√© : sarimax_reports/report_voltage.txt\n",
            "\n",
            "==================== RAPPORT COMPLET POUR : HUMIDITY ====================\n",
            "   -> Analyse de la s√©rie brute pour 'humidity' termin√©e.\n",
            "   -> Analyse du mod√®le pour 'humidity' termin√©e.\n",
            "   -> Rapport texte sauvegard√© : sarimax_reports/report_humidity.txt\n",
            "\n",
            "==================== RAPPORT COMPLET POUR : DISSOLVED_GAS ====================\n",
            "   -> Analyse de la s√©rie brute pour 'dissolved_gas' termin√©e.\n",
            "   -> Analyse du mod√®le pour 'dissolved_gas' termin√©e.\n",
            "   -> Rapport texte sauvegard√© : sarimax_reports/report_dissolved_gas.txt\n",
            "\n",
            "‚úÖ Tous les rapports ont √©t√© g√©n√©r√©s avec succ√®s dans le dossier 'sarimax_reports'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UdKHgctJJq46",
        "outputId": "1c46e7cf-3c41-4774-9dfa-0131fe21b342"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting Dynamic Model Training ---\n",
            "Loading data from 'datac.csv'...\n",
            "Data preparation complete.\n",
            "\n",
            "--- Training model for: temperature_oil ---\n",
            "Target: temperature_oil\n",
            "Features: ['temperature_winding', 'current', 'voltage', 'humidity', 'dissolved_gas']\n",
            "Training SARIMAX for temperature_oil... (This may take a few minutes)\n",
            "Training complete.\n",
            "Saving model to 'trained_models/model_temperature_oil.pkl'...\n",
            "\n",
            "--- Training model for: temperature_winding ---\n",
            "Target: temperature_winding\n",
            "Features: ['temperature_oil', 'current', 'voltage', 'humidity', 'dissolved_gas']\n",
            "Training SARIMAX for temperature_winding... (This may take a few minutes)\n",
            "Training complete.\n",
            "Saving model to 'trained_models/model_temperature_winding.pkl'...\n",
            "\n",
            "--- Training model for: current ---\n",
            "Target: current\n",
            "Features: ['temperature_oil', 'temperature_winding', 'voltage', 'humidity', 'dissolved_gas']\n",
            "Training SARIMAX for current... (This may take a few minutes)\n",
            "Training complete.\n",
            "Saving model to 'trained_models/model_current.pkl'...\n",
            "\n",
            "--- Training model for: voltage ---\n",
            "Target: voltage\n",
            "Features: ['temperature_oil', 'temperature_winding', 'current', 'humidity', 'dissolved_gas']\n",
            "Training SARIMAX for voltage... (This may take a few minutes)\n",
            "Training complete.\n",
            "Saving model to 'trained_models/model_voltage.pkl'...\n",
            "\n",
            "--- Training model for: humidity ---\n",
            "Target: humidity\n",
            "Features: ['temperature_oil', 'temperature_winding', 'current', 'voltage', 'dissolved_gas']\n",
            "Training SARIMAX for humidity... (This may take a few minutes)\n",
            "Training complete.\n",
            "Saving model to 'trained_models/model_humidity.pkl'...\n",
            "\n",
            "--- Training model for: dissolved_gas ---\n",
            "Target: dissolved_gas\n",
            "Features: ['temperature_oil', 'temperature_winding', 'current', 'voltage', 'humidity']\n",
            "Training SARIMAX for dissolved_gas... (This may take a few minutes)\n",
            "Training complete.\n",
            "Saving model to 'trained_models/model_dissolved_gas.pkl'...\n",
            "\n",
            "--- All models have been successfully trained and saved! ---\n"
          ]
        }
      ],
      "source": [
        "# train_models.py\n",
        "\n",
        "import pandas as pd\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "import pickle\n",
        "import warnings\n",
        "import os\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "print(\"--- Starting Dynamic Model Training ---\")\n",
        "\n",
        "# --- Configuration ---\n",
        "file_path = 'datac.csv'\n",
        "\n",
        "# Define ALL columns that could potentially be a target or a feature.\n",
        "# We will exclude the 'fault' column as it seems categorical.\n",
        "all_numeric_columns = [\n",
        "    'temperature_oil',\n",
        "    'temperature_winding',\n",
        "    'current',\n",
        "    'voltage',\n",
        "    'humidity',\n",
        "    'dissolved_gas'\n",
        "]\n",
        "time_column = 'timestamp'\n",
        "\n",
        "# The directory where models will be saved\n",
        "model_dir = 'trained_models'\n",
        "# --------------------\n",
        "\n",
        "# Create the model directory if it doesn't exist\n",
        "if not os.path.exists(model_dir):\n",
        "    os.makedirs(model_dir)\n",
        "    print(f\"Created directory: '{model_dir}'\")\n",
        "\n",
        "# 1. Load and prepare the entire dataset\n",
        "print(f\"Loading data from '{file_path}'...\")\n",
        "try:\n",
        "    df = pd.read_csv(file_path, usecols=[time_column] + all_numeric_columns)\n",
        "except (FileNotFoundError, ValueError) as e:\n",
        "    print(f\"Error loading data: {e}\")\n",
        "    exit()\n",
        "\n",
        "df[time_column] = pd.to_datetime(df[time_column])\n",
        "df = df.set_index(time_column).sort_index()\n",
        "\n",
        "# Handle missing values\n",
        "if df.isnull().values.any():\n",
        "    df.fillna(method='ffill', inplace=True)\n",
        "    df.fillna(method='bfill', inplace=True)\n",
        "print(\"Data preparation complete.\")\n",
        "\n",
        "\n",
        "# 2. Loop through each target column to train and save a model\n",
        "for target_column in all_numeric_columns:\n",
        "    print(f\"\\n--- Training model for: {target_column} ---\")\n",
        "\n",
        "    # The target variable (y) is the current column in the loop\n",
        "    y_train = df[target_column]\n",
        "\n",
        "    # The exogenous variables (exog) are all OTHER numeric columns\n",
        "    exog_columns = [col for col in all_numeric_columns if col != target_column]\n",
        "    exog_train = df[exog_columns]\n",
        "\n",
        "    print(f\"Target: {target_column}\")\n",
        "    print(f\"Features: {exog_columns}\")\n",
        "\n",
        "    # Define SARIMAX model parameters\n",
        "    my_order = (1, 1, 1)\n",
        "    my_seasonal_order = (1, 1, 1, 24)\n",
        "\n",
        "    # Train the SARIMAX model\n",
        "    print(f\"Training SARIMAX for {target_column}... (This may take a few minutes)\")\n",
        "    model = SARIMAX(y_train, exog=exog_train, order=my_order, seasonal_order=my_seasonal_order)\n",
        "    model_fit = model.fit(disp=False)\n",
        "    print(\"Training complete.\")\n",
        "\n",
        "    # 3. Save the trained model to a unique file\n",
        "    model_save_path = os.path.join(model_dir, f'model_{target_column}.pkl')\n",
        "    print(f\"Saving model to '{model_save_path}'...\")\n",
        "    with open(model_save_path, 'wb') as pkl_file:\n",
        "        pickle.dump(model_fit, pkl_file)\n",
        "\n",
        "print(\"\\n--- All models have been successfully trained and saved! ---\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# predict_from_model.py\n",
        "\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import warnings\n",
        "import os\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# --- Configuration ---\n",
        "file_path = 'datac.csv'\n",
        "model_dir = 'trained_models'\n",
        "all_numeric_columns = [\n",
        "    'temperature_oil',\n",
        "    'temperature_winding',\n",
        "    'current',\n",
        "    'voltage',\n",
        "    'humidity',\n",
        "    'dissolved_gas'\n",
        "]\n",
        "time_column = 'timestamp'\n",
        "# --------------------\n",
        "\n",
        "# --- 1. SETUP: LOAD HISTORICAL DATA AND PRE-LOAD MODELS (Optional but efficient) ---\n",
        "print(\"--- Initializing Dynamic Prediction Service ---\")\n",
        "\n",
        "# We can pre-load all models into a dictionary for faster access\n",
        "loaded_models = {}\n",
        "for column_name in all_numeric_columns:\n",
        "    model_path = os.path.join(model_dir, f'model_{column_name}.pkl')\n",
        "    try:\n",
        "        with open(model_path, 'rb') as f:\n",
        "            loaded_models[column_name] = pickle.load(f)\n",
        "        print(f\"Successfully loaded model for '{column_name}'\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Warning: Model file not found at '{model_path}'. Cannot predict for this column.\")\n",
        "\n",
        "# Load historical data for context (last values, past data checks)\n",
        "try:\n",
        "    df_history = pd.read_csv(file_path, usecols=[time_column] + all_numeric_columns)\n",
        "    df_history[time_column] = pd.to_datetime(df_history[time_column])\n",
        "    df_history = df_history.set_index(time_column).sort_index()\n",
        "    last_known_timestamp = df_history.index[-1]\n",
        "    print(\"\\nHistorical data loaded for context.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"FATAL ERROR: Historical data file '{file_path}' not found. Cannot proceed.\")\n",
        "    exit()\n",
        "\n",
        "print(\"--- Service is ready for predictions. ---\\n\")\n",
        "\n",
        "\n",
        "# --- 2. DYNAMIC PREDICTION FUNCTION ---\n",
        "def predict_value_at(target_column: str, timestamp_str: str) -> float:\n",
        "    \"\"\"Predicts a value for a specific column and timestamp using the correct pre-loaded model.\"\"\"\n",
        "\n",
        "    # Validate the requested target column\n",
        "    if target_column not in loaded_models:\n",
        "        print(f\"Error: No model available for '{target_column}'. Please train it first.\")\n",
        "        return None\n",
        "\n",
        "    model = loaded_models[target_column]\n",
        "\n",
        "    try:\n",
        "        target_timestamp = pd.to_datetime(timestamp_str)\n",
        "    except ValueError:\n",
        "        print(f\"Error: Invalid date format for '{timestamp_str}'. Use 'YYYY-MM-DD HH:MM:SS'.\")\n",
        "        return None\n",
        "\n",
        "    # Handle requests for dates that are in our historical data\n",
        "    if target_timestamp <= last_known_timestamp:\n",
        "        print(f\"Info: '{timestamp_str}' is in the past. Returning known value for {target_column}.\")\n",
        "        return df_history.asof(target_timestamp)[target_column]\n",
        "\n",
        "    # For future predictions, we need the exogenous variables (all columns EXCEPT the target)\n",
        "    exog_cols = [col for col in all_numeric_columns if col != target_column]\n",
        "    last_exog_values = df_history[exog_cols].iloc[-1:]\n",
        "\n",
        "    # Forecast into the future\n",
        "    forecast_result = model.get_forecast(until=target_timestamp, exog=last_exog_values)\n",
        "    predicted_value = forecast_result.predicted_mean.iloc[-1]\n",
        "\n",
        "    return predicted_value\n",
        "\n",
        "\n",
        "# --- 3. TEST THE FUNCTION ---\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"--- Testing the Dynamic Prediction Function ---\\n\")\n",
        "\n",
        "    # Define test cases: (column_to_predict, time_to_predict)\n",
        "    test_cases = [\n",
        "        ('temperature_oil', (last_known_timestamp + pd.Timedelta(hours=6)).strftime('%Y-%m-%d %H:%M:%S')),\n",
        "        ('current', (last_known_timestamp + pd.Timedelta(hours=24)).strftime('%Y-%m-%d %H:%M:%S')),\n",
        "        ('voltage', (last_known_timestamp + pd.Timedelta(hours=48)).strftime('%Y-%m-%d %H:%M:%S')),\n",
        "        ('humidity', (last_known_timestamp + pd.Timedelta(hours=72)).strftime('%Y-%m-%d %H:%M:%S')),\n",
        "        ('temperature_winding', (last_known_timestamp + pd.Timedelta(hours=1)).strftime('%Y-%m-%d %H:%M:%S')),\n",
        "    ]\n",
        "\n",
        "    for column, ts_str in test_cases:\n",
        "        predicted_val = predict_value_at(column, ts_str)\n",
        "        if predicted_val is not None:\n",
        "            # Get the units for better print statements\n",
        "            units = \"V\" if \"volt\" in column else \"A\" if \"curr\" in column else \"%\" if \"hum\" in column else \"C\" if \"temp\" in column else \"\"\n",
        "            print(f\"Prediction for '{column}' at '{ts_str}': {predicted_val:.2f} {units}\")\n",
        "        print(\"-\" * 20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zy7kb92SKO7Z",
        "outputId": "20f38ef0-7930-4caf-d7b6-07e749e08728"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Initializing Dynamic Prediction Service ---\n",
            "Successfully loaded model for 'temperature_oil'\n",
            "Successfully loaded model for 'temperature_winding'\n",
            "Successfully loaded model for 'current'\n",
            "Successfully loaded model for 'voltage'\n",
            "Successfully loaded model for 'humidity'\n",
            "Successfully loaded model for 'dissolved_gas'\n",
            "\n",
            "Historical data loaded for context.\n",
            "--- Service is ready for predictions. ---\n",
            "\n",
            "--- Testing the Dynamic Prediction Function ---\n",
            "\n",
            "Prediction for 'temperature_oil' at '2023-03-25 13:00:00': 59.34 C\n",
            "--------------------\n",
            "Prediction for 'current' at '2023-03-26 07:00:00': 249.55 A\n",
            "--------------------\n",
            "Prediction for 'voltage' at '2023-03-27 07:00:00': 21934.17 V\n",
            "--------------------\n",
            "Prediction for 'humidity' at '2023-03-28 07:00:00': 36.91 %\n",
            "--------------------\n",
            "Prediction for 'temperature_winding' at '2023-03-25 08:00:00': 67.27 C\n",
            "--------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train_classifier.py (version robuste pour pr√©diction √† J+1)\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import joblib\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# === 1. Configuration ===\n",
        "\n",
        "FEATURES_COLS = [\n",
        "    'temperature_oil', 'temperature_winding', 'current',\n",
        "    'voltage', 'humidity', 'dissolved_gas'\n",
        "]\n",
        "TIMESTAMP_COL = 'timestamp'\n",
        "FAULT_COL = 'fault'\n",
        "TARGET_COL = 'Panne_Le_Lendemain'\n",
        "\n",
        "DATA_FILE = 'datac.csv'  # Assure-toi d'utiliser le fichier nettoy√©\n",
        "OUTPUT_MODEL_PATH = 'trained_models/classifier_model_j1.pkl'\n",
        "DETECTION_HORIZON_DAYS = 1\n",
        "\n",
        "# === 2. Chargement des donn√©es ===\n",
        "\n",
        "print(f\"üì• Chargement des donn√©es depuis : {DATA_FILE}\")\n",
        "try:\n",
        "    df = pd.read_csv(DATA_FILE, parse_dates=[TIMESTAMP_COL])\n",
        "except FileNotFoundError:\n",
        "    print(f\"‚ùå Fichier non trouv√© : {DATA_FILE}\")\n",
        "    exit()\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Erreur de chargement : {e}\")\n",
        "    exit()\n",
        "\n",
        "# V√©rifier la pr√©sence des colonnes n√©cessaires\n",
        "required_columns = FEATURES_COLS + [FAULT_COL, TIMESTAMP_COL]\n",
        "missing_cols = [col for col in required_columns if col not in df.columns]\n",
        "if missing_cols:\n",
        "    print(f\"‚ùå Colonnes manquantes dans le fichier : {missing_cols}\")\n",
        "    exit()\n",
        "\n",
        "# === 3. Pr√©paration de la cible √† J+1 ===\n",
        "\n",
        "print(f\"üõ†Ô∏è Cr√©ation de la variable cible '{TARGET_COL}' (d√©calage -{DETECTION_HORIZON_DAYS})...\")\n",
        "df[TARGET_COL] = df[FAULT_COL].shift(-DETECTION_HORIZON_DAYS, fill_value=0).astype(int)\n",
        "\n",
        "X = df[FEATURES_COLS]\n",
        "y = df[TARGET_COL]\n",
        "\n",
        "# === 4. Affichage de la distribution des classes ===\n",
        "\n",
        "print(\"\\nüìä Distribution des classes (0 = pas de panne, 1 = panne demain) :\")\n",
        "print(y.value_counts(normalize=True).rename(lambda x: f\"{x} - {'Panne' if x == 1 else 'OK'}\"))\n",
        "\n",
        "# === 5. Split entra√Ænement / test (80% - 20%) ===\n",
        "\n",
        "split_index = int(len(df) * 0.8)\n",
        "X_train, X_test = X[:split_index], X[split_index:]\n",
        "y_train, y_test = y[:split_index], y[split_index:]\n",
        "\n",
        "print(f\"\\nüìö Entra√Ænement sur {len(X_train)} exemples, test sur {len(X_test)}.\")\n",
        "\n",
        "# === 6. Entra√Ænement du mod√®le ===\n",
        "\n",
        "model = RandomForestClassifier(\n",
        "    n_estimators=150,\n",
        "    class_weight='balanced',\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "print(\"\\n‚öôÔ∏è Entra√Ænement du mod√®le RandomForest...\")\n",
        "model.fit(X_train, y_train)\n",
        "print(\"‚úÖ Entra√Ænement termin√©.\")\n",
        "\n",
        "# === 7. √âvaluation ===\n",
        "\n",
        "print(\"\\nüìà √âvaluation sur l'ensemble de test :\")\n",
        "y_pred = model.predict(X_test)\n",
        "print(classification_report(y_test, y_pred, target_names=['Pas de Panne (0)', 'Panne (1)']))\n",
        "\n",
        "# === 8. Sauvegarde ===\n",
        "\n",
        "# Cr√©e le dossier s'il n'existe pas\n",
        "os.makedirs(os.path.dirname(OUTPUT_MODEL_PATH), exist_ok=True)\n",
        "\n",
        "print(f\"\\nüíæ Sauvegarde du mod√®le vers : {OUTPUT_MODEL_PATH}\")\n",
        "joblib.dump(model, OUTPUT_MODEL_PATH)\n",
        "\n",
        "print(\"\\n‚úÖ Script termin√© avec succ√®s ! Mod√®le J+1 pr√™t √† √™tre utilis√©.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lKnZ0HrMNMqI",
        "outputId": "56c4a775-617e-46e3-d37e-044c1d4e4a7a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì• Chargement des donn√©es depuis : datac.csv\n",
            "üõ†Ô∏è Cr√©ation de la variable cible 'Panne_Le_Lendemain' (d√©calage -1)...\n",
            "\n",
            "üìä Distribution des classes (0 = pas de panne, 1 = panne demain) :\n",
            "Panne_Le_Lendemain\n",
            "0 - OK       0.953\n",
            "1 - Panne    0.047\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "üìö Entra√Ænement sur 1600 exemples, test sur 400.\n",
            "\n",
            "‚öôÔ∏è Entra√Ænement du mod√®le RandomForest...\n",
            "‚úÖ Entra√Ænement termin√©.\n",
            "\n",
            "üìà √âvaluation sur l'ensemble de test :\n",
            "                  precision    recall  f1-score   support\n",
            "\n",
            "Pas de Panne (0)       0.94      1.00      0.97       378\n",
            "       Panne (1)       0.00      0.00      0.00        22\n",
            "\n",
            "        accuracy                           0.94       400\n",
            "       macro avg       0.47      0.50      0.49       400\n",
            "    weighted avg       0.89      0.94      0.92       400\n",
            "\n",
            "\n",
            "üíæ Sauvegarde du mod√®le vers : trained_models/classifier_model_j1.pkl\n",
            "\n",
            "‚úÖ Script termin√© avec succ√®s ! Mod√®le J+1 pr√™t √† √™tre utilis√©.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#pour telecharger un fichier dans google colab\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "stHhLSWr85N_",
        "outputId": "a6b07f00-ca76-4993-ca93-40be77ab4948"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-71c024ba-d919-4e53-942b-02a7a606d936\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-71c024ba-d919-4e53-942b-02a7a606d936\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving predict.py to predict.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# predict_failure_pipeline.py (Nouveau script qui remplace predict_classifier.py et predict.py)\n",
        "\n",
        "import pandas as pd\n",
        "import joblib\n",
        "from predict import predict_value_at # On importe la fonction du script SARIMAX\n",
        "\n",
        "# === 1. Configuration ===\n",
        "CLASSIFIER_MODEL_PATH = 'trained_models/classifier_model_j1.pkl'\n",
        "FEATURES_COLS = [\n",
        "    'temperature_oil', 'temperature_winding', 'current',\n",
        "    'voltage', 'humidity', 'dissolved_gas'\n",
        "]\n",
        "# Horizon de pr√©diction : 24 heures dans le futur\n",
        "prediction_horizon = pd.Timedelta(hours=24)\n",
        "last_known_timestamp = pd.to_datetime(pd.read_csv('datac.csv')['timestamp'].iloc[-1])\n",
        "target_timestamp = last_known_timestamp + prediction_horizon\n",
        "target_timestamp_str = target_timestamp.strftime('%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "# === 2. Chargement du classifieur ===\n",
        "print(f\"üì• Chargement du mod√®le classifieur depuis : {CLASSIFIER_MODEL_PATH}\")\n",
        "classifier_model = joblib.load(CLASSIFIER_MODEL_PATH)\n",
        "\n",
        "# === 3. Construction du vecteur de caract√©ristiques du futur via SARIMAX ===\n",
        "print(f\"\\nüîÆ Construction des donn√©es pour le futur ({target_timestamp_str}) en utilisant les mod√®les SARIMAX...\")\n",
        "future_data = {}\n",
        "for feature in FEATURES_COLS:\n",
        "    print(f\"  -> Pr√©diction de '{feature}'...\")\n",
        "    predicted_value = predict_value_at(feature, target_timestamp_str)\n",
        "    if predicted_value is None:\n",
        "        print(f\"‚ùå Impossible de pr√©dire '{feature}', arr√™t du processus.\")\n",
        "        exit()\n",
        "    future_data[feature] = predicted_value\n",
        "\n",
        "# Convertir le dictionnaire en DataFrame d'une ligne\n",
        "X_future = pd.DataFrame([future_data])\n",
        "\n",
        "print(\"\\nüìä Donn√©es futures pr√©dites pour le classifieur :\")\n",
        "print(X_future)\n",
        "\n",
        "# === 4. Pr√©diction finale de la panne ===\n",
        "prediction = classifier_model.predict(X_future)[0]\n",
        "proba = classifier_model.predict_proba(X_future)[0][prediction]\n",
        "\n",
        "# === 5. R√©sultat ===\n",
        "if prediction == 1:\n",
        "    print(f\"\\nüö® Pr√©diction FINALE : **Panne probable √† {target_timestamp_str}** avec une confiance de {proba:.2%}\")\n",
        "else:\n",
        "    print(f\"\\n‚úÖ Pr√©diction FINALE : **Pas de panne pr√©vue √† {target_timestamp_str}** avec une confiance de {proba:.2%}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mwm75sviNWzx",
        "outputId": "53cc449a-1000-4253-8270-9c22b3ed6082"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Initializing Dynamic Prediction Service ---\n",
            "Successfully loaded model for 'temperature_oil'\n",
            "Successfully loaded model for 'temperature_winding'\n",
            "Successfully loaded model for 'current'\n",
            "Successfully loaded model for 'voltage'\n",
            "Successfully loaded model for 'humidity'\n",
            "Successfully loaded model for 'dissolved_gas'\n",
            "\n",
            "Historical data loaded for context.\n",
            "--- Service is ready for predictions. ---\n",
            "\n",
            "üì• Chargement du mod√®le classifieur depuis : trained_models/classifier_model_j1.pkl\n",
            "\n",
            "üîÆ Construction des donn√©es pour le futur (2023-03-26 07:00:00) en utilisant les mod√®les SARIMAX...\n",
            "  -> Pr√©diction de 'temperature_oil'...\n",
            "  -> Pr√©diction de 'temperature_winding'...\n",
            "  -> Pr√©diction de 'current'...\n",
            "  -> Pr√©diction de 'voltage'...\n",
            "  -> Pr√©diction de 'humidity'...\n",
            "  -> Pr√©diction de 'dissolved_gas'...\n",
            "\n",
            "üìä Donn√©es futures pr√©dites pour le classifieur :\n",
            "   temperature_oil  temperature_winding     current       voltage   humidity  \\\n",
            "0        59.343341            67.265554  249.549189  21934.171041  36.914963   \n",
            "\n",
            "   dissolved_gas  \n",
            "0      45.951133  \n",
            "\n",
            "‚úÖ Pr√©diction FINALE : **Pas de panne pr√©vue √† 2023-03-26 07:00:00** avec une confiance de 95.33%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "import warnings\n",
        "import numpy as np\n",
        "\n",
        "# Suppress warnings for cleaner output\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# --- 1. SETUP PHASE: TRAIN THE MODEL ONCE ---\n",
        "\n",
        "print(\"--- Starting Setup Phase ---\")\n",
        "\n",
        "# Configuration\n",
        "file_path = 'datac.csv'\n",
        "time_column = 'timestamp'\n",
        "target_column = 'temperature_oil'\n",
        "exog_columns = ['current', 'voltage', 'humidity', 'dissolved_gas']\n",
        "\n",
        "# Load and prepare the entire dataset\n",
        "print(f\"Loading data from '{file_path}'...\")\n",
        "try:\n",
        "    columns_to_load = [time_column, target_column] + exog_columns\n",
        "    df = pd.read_csv(file_path, usecols=columns_to_load)\n",
        "except (FileNotFoundError, ValueError) as e:\n",
        "    print(f\"Error loading data: {e}\")\n",
        "    exit()\n",
        "\n",
        "df[time_column] = pd.to_datetime(df[time_column])\n",
        "df = df.set_index(time_column).sort_index()\n",
        "\n",
        "# Handle missing values\n",
        "if df.isnull().values.any():\n",
        "    print(f\"Found {df.isnull().values.sum()} missing values. Filling them...\")\n",
        "    df.fillna(method='ffill', inplace=True)\n",
        "    df.fillna(method='bfill', inplace=True)\n",
        "\n",
        "print(\"Data preparation complete.\")\n",
        "\n",
        "# Separate the target (y) and exogenous (exog) variables\n",
        "y_full = df[target_column]\n",
        "exog_full = df[exog_columns]\n",
        "\n",
        "# Define SARIMAX model parameters\n",
        "my_order = (1, 1, 1)\n",
        "my_seasonal_order = (1, 1, 1, 24)\n",
        "\n",
        "# Train the SARIMAX model on the full dataset\n",
        "print(\"Training the predictive model on all available data... (This may take a few minutes)\")\n",
        "model_full = SARIMAX(y_full, exog=exog_full, order=my_order, seasonal_order=my_seasonal_order)\n",
        "model_fit_full = model_full.fit(disp=False)\n",
        "print(\"Model training complete.\")\n",
        "print(\"--- Setup Phase Finished. Ready for predictions. ---\\n\")\n",
        "\n",
        "\n",
        "# --- 2. PREDICTION FUNCTION ---\n",
        "\n",
        "def predict_temperature_at(timestamp_str: str) -> float:\n",
        "    \"\"\"\n",
        "    Predicts the temperature for a specific future timestamp.\n",
        "\n",
        "    Args:\n",
        "        timestamp_str (str): The date and time for the prediction,\n",
        "                             in a format like 'YYYY-MM-DD HH:MM:SS'.\n",
        "\n",
        "    Returns:\n",
        "        float: The predicted temperature value.\n",
        "    \"\"\"\n",
        "    global model_fit_full, df, exog_full\n",
        "\n",
        "    # Convert input string to a datetime object\n",
        "    try:\n",
        "        target_timestamp = pd.to_datetime(timestamp_str)\n",
        "    except ValueError:\n",
        "        print(f\"Error: Invalid date format for '{timestamp_str}'. Please use 'YYYY-MM-DD HH:MM:SS'.\")\n",
        "        return None\n",
        "\n",
        "    last_known_timestamp = df.index[-1]\n",
        "\n",
        "    # Check if the requested time is in the past (already has data)\n",
        "    if target_timestamp <= last_known_timestamp:\n",
        "        print(f\"Info: '{timestamp_str}' is in the past. Returning actual known value.\")\n",
        "        # Return the closest known value if it exists\n",
        "        try:\n",
        "            return df.loc[target_timestamp, target_column]\n",
        "        except KeyError:\n",
        "            # Use asof for approximate match if exact timestamp is not in index\n",
        "            return df[target_column].asof(target_timestamp)\n",
        "\n",
        "\n",
        "    # If the time is in the future, we need to forecast\n",
        "    # Determine the number of steps to forecast\n",
        "    # The frequency of the data is assumed to be hourly based on the seasonal order (24) and prior context\n",
        "    freq = pd.infer_freq(df.index)\n",
        "    if freq is None:\n",
        "        print(\"Warning: Could not infer frequency of the time series. Assuming hourly ('H').\")\n",
        "        freq = 'H'\n",
        "\n",
        "    time_difference = target_timestamp - last_known_timestamp\n",
        "    # Calculate the number of steps based on the inferred frequency\n",
        "    steps = int(time_difference / pd.Timedelta(freq))\n",
        "\n",
        "    if steps <= 0:\n",
        "         print(f\"Info: Target timestamp '{target_timestamp}' is not in the future relative to the last known timestamp '{last_known_timestamp}'.\")\n",
        "         return df[target_column].iloc[-1]\n",
        "\n",
        "\n",
        "    # We need to provide future values for the helper variables (exog)\n",
        "    # A common assumption is that they remain constant at their last known value.\n",
        "    last_exog_values = exog_full.iloc[-1:].values\n",
        "\n",
        "    # Repeat the last known exogenous values for the number of forecast steps\n",
        "    exog_forecast = np.tile(last_exog_values, (steps, 1))\n",
        "\n",
        "\n",
        "    # Generate a forecast from the end of our data up to the target time\n",
        "    forecast_result = model_fit_full.get_forecast(\n",
        "        steps=steps, # Use steps instead of until\n",
        "        exog=exog_forecast # Provide exog for all forecast steps\n",
        "    )\n",
        "\n",
        "    # The result contains predictions for the whole interval. We just need the one at our target time.\n",
        "    # The forecast result index will be the future timestamps\n",
        "    predicted_value = forecast_result.predicted_mean.iloc[-1] # Get the last predicted value\n",
        "\n",
        "    return predicted_value\n",
        "\n",
        "\n",
        "# --- 3. TEST THE FUNCTION ---\n",
        "\n",
        "print(\"--- Testing the Prediction Function ---\")\n",
        "\n",
        "# Get the last timestamp from our data to generate future test dates\n",
        "last_date_in_data = df.index[-1]\n",
        "\n",
        "# Generate 10 random future timestamps within the next 7 days for testing\n",
        "# (1 to 168 hours ahead)\n",
        "test_dates = [last_date_in_data + pd.Timedelta(hours=h) for h in sorted(pd.Series(range(1, 169)).sample(10))]\n",
        "test_date_strings = [dt.strftime('%Y-%m-%d %H:%M:%S') for dt in test_dates]\n",
        "\n",
        "# Add one test date from the past to show that functionality\n",
        "test_date_strings.append(df.index[100].strftime('%Y-%m-%d %H:%M:%S')) # Add a date from the past that is in the index\n",
        "\n",
        "for ts_str in test_date_strings:\n",
        "    predicted_temp = predict_temperature_at(ts_str)\n",
        "    if predicted_temp is not None:\n",
        "        print(f\"Prediction for '{ts_str}': {predicted_temp:.2f} degrees\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 540
        },
        "id": "kag7vG_gPnTV",
        "outputId": "e20ab89d-9462-4267-8eb4-57fcdda15dcb"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting Setup Phase ---\n",
            "Loading data from 'datac.csv'...\n",
            "Data preparation complete.\n",
            "Training the predictive model on all available data... (This may take a few minutes)\n",
            "Model training complete.\n",
            "--- Setup Phase Finished. Ready for predictions. ---\n",
            "\n",
            "--- Testing the Prediction Function ---\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "unit abbreviation w/o a number",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-17-1978840046.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mts_str\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_date_strings\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m     \u001b[0mpredicted_temp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_temperature_at\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mts_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpredicted_temp\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Prediction for '{ts_str}': {predicted_temp:.2f} degrees\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-17-1978840046.py\u001b[0m in \u001b[0;36mpredict_temperature_at\u001b[0;34m(timestamp_str)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0mtime_difference\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_timestamp\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlast_known_timestamp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;31m# Calculate the number of steps based on the inferred frequency\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m     \u001b[0msteps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_difference\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTimedelta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msteps\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mtimedeltas.pyx\u001b[0m in \u001b[0;36mpandas._libs.tslibs.timedeltas.Timedelta.__new__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mtimedeltas.pyx\u001b[0m in \u001b[0;36mpandas._libs.tslibs.timedeltas.parse_timedelta_string\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: unit abbreviation w/o a number"
          ]
        }
      ]
    }
  ]
}